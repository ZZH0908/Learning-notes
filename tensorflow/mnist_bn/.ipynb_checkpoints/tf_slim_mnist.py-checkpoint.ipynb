{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model():\n",
    "    # Create the model\n",
    "    x = tf.placeholder(tf.float32, [None, 784])\n",
    "    keep_prob = tf.placeholder(tf.float32, [])\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                        activation_fn=tf.nn.relu,\n",
    "                        normalizer_fn=slim.batch_norm,\n",
    "                        normalizer_params={'is_training': is_training, 'decay': 0.95}):\n",
    "        conv1 = slim.conv2d(x_image, 16, [5, 5], scope='conv1')\n",
    "        pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool1')\n",
    "        conv2 = slim.conv2d(pool1, 32, [5, 5], scope='conv2')\n",
    "        pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool2')\n",
    "        flatten = slim.flatten(pool2)\n",
    "        fc = slim.fully_connected(flatten, 1024, scope='fc1')\n",
    "        drop = slim.dropout(fc, keep_prob=keep_prob)\n",
    "        logits = slim.fully_connected(drop, 10, activation_fn=None, scope='logits')\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=logits))\n",
    "\n",
    "    step = tf.get_variable(\"step\", [], initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    train_step = slim.learning.create_train_op(cross_entropy, optimizer, global_step=step)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    if update_ops:\n",
    "        updates = tf.group(*update_ops)\n",
    "        cross_entropy = control_flow_ops.with_dependencies([updates], cross_entropy)\n",
    "\n",
    "    # if update_ops:\n",
    "    #     updates = tf.group(*update_ops)\n",
    "    #     cross_entropy = control_flow_ops.with_dependencies([updates], train_step)\n",
    "\n",
    "    # with tf.control_dependencies([tf.group(*update_ops)]):\n",
    "    #     train_step = slim.learning.create_train_op(cross_entropy, optimizer, global_step=step)\n",
    "\n",
    "    # Add summaries for BN variables\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    # for v in tf.all_variables():\n",
    "    for v in tf.global_variables():\n",
    "        # 检查某字符串是否以指定字符串开头，返回True或False\n",
    "        if v.name.startswith('conv1/Batch') or v.name.startswith('conv2/Batch') or \\\n",
    "                v.name.startswith('fc1/Batch') or v.name.startswith('logits/Batch'):\n",
    "            print(v.name)\n",
    "            tf.summary.histogram(v.name, v)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "    return {'x': x,\n",
    "            'y_': y_,\n",
    "            'keep_prob': keep_prob,\n",
    "            'is_training': is_training,\n",
    "            'train_step': train_step,\n",
    "            'global_step': step,\n",
    "            'accuracy': accuracy,\n",
    "            'cross_entropy': cross_entropy,\n",
    "            'summary': merged_summary_op}\n",
    "\n",
    "\n",
    "def train():\n",
    "    # clear checkpoint directory\n",
    "    print('Clearing existed checkpoints and logs')\n",
    "    \n",
    "    mnist = input_data.read_data_sets(FLAGS['data_dir'], one_hot=True)\n",
    "    net = model()\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(FLAGS['train_log_dir'], 'train'), sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter(os.path.join(FLAGS['train_log_dir'], 'valid'), sess.graph)\n",
    "\n",
    "    # Train\n",
    "    batch_size = FLAGS['batch_size']\n",
    "    for i in range(1000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        train_dict = {net['x']: batch_xs,\n",
    "                      net['y_']: batch_ys,\n",
    "                      net['keep_prob']: 0.5,\n",
    "                      net['is_training']: True}\n",
    "        step, _ = sess.run([net['global_step'], net['train_step']], feed_dict=train_dict)\n",
    "        if step % 50 == 0:\n",
    "            train_dict = {net['x']: batch_xs,\n",
    "                          net['y_']: batch_ys,\n",
    "                          net['keep_prob']: 1.0,\n",
    "                          net['is_training']: True}\n",
    "            entropy, acc, summary = sess.run([net['cross_entropy'], net['accuracy'], net['summary']],\n",
    "                                             feed_dict=train_dict)\n",
    "            train_writer.add_summary(summary, global_step=step)\n",
    "            print('Train step {}: entropy {}: accuracy {}'.format(step, entropy, acc))\n",
    "\n",
    "            # Note: the validation error is erratic in the beginning (Maybe 2~3k steps).\n",
    "            # This does NOT imply the batch normalization is buggy.\n",
    "            # On the contrary, it's BN's dynamics: moving_mean/variance are not estimated that well in the beginning.\n",
    "            valid_dict = {net['x']: batch_xs,\n",
    "                          net['y_']: batch_ys,\n",
    "                          net['keep_prob']: 1.0,\n",
    "                          net['is_training']: True}\n",
    "            entropy, acc, summary = sess.run([net['cross_entropy'], net['accuracy'], net['summary']],\n",
    "                                             feed_dict=valid_dict)\n",
    "            valid_writer.add_summary(summary, global_step=step)\n",
    "            print('***** Valid step {}: entropy {}: accuracy {} *****'.format(step, entropy, acc))\n",
    "    saver.save(sess, os.path.join(FLAGS['checkpoint_dir'], 'mnist-conv-slim'))\n",
    "    print('Finish training')\n",
    "\n",
    "    # validation\n",
    "    acc = 0.0\n",
    "    batch_size = FLAGS['batch_size']\n",
    "    num_iter = 5000 // batch_size\n",
    "    for i in range(num_iter):\n",
    "        batch_xs, batch_ys = mnist.validation.next_batch(batch_size)\n",
    "        test_dict = {net['x']: batch_xs,\n",
    "                     net['y_']: batch_ys,\n",
    "                     net['keep_prob']: 1.0,\n",
    "                     net['is_training']: False}\n",
    "        acc_ = sess.run(net['accuracy'], feed_dict=test_dict)\n",
    "        acc += acc_\n",
    "    print('Overall validation accuracy {}'.format(acc / num_iter))\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "def test():\n",
    "    mnist = input_data.read_data_sets(FLAGS['data_dir'], one_hot=True)\n",
    "    # Test trained model\n",
    "    net = model()\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.latest_checkpoint(FLAGS['checkpoint_dir'])\n",
    "    if ckpt:\n",
    "        saver.restore(sess, ckpt)\n",
    "        print(\"restore from the checkpoint {0}\".format(ckpt))\n",
    "\n",
    "    acc = 0.0\n",
    "    batch_size = FLAGS['batch_size']\n",
    "    num_iter = 10000 // batch_size\n",
    "    for i in range(num_iter):\n",
    "        batch_xs, batch_ys = mnist.test.next_batch(batch_size)\n",
    "        feed_dict = {net['x']: batch_xs,\n",
    "                     net['y_']: batch_ys,\n",
    "                     net['keep_prob']: 1.0,\n",
    "                     net['is_training']: False}\n",
    "        acc_ = sess.run(net['accuracy'], feed_dict=feed_dict)\n",
    "        acc += acc_\n",
    "    print('Overall test accuracy {}'.format(acc / num_iter))\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "FLAGS = {'data_dir':'E:/database/mnist/', 'batch_size':64, 'train_log_dir':'E:/vscode/test/log/', \n",
    "        'checkpoint_dir':'E:/vscode/test/checkpoint/'}\n",
    "\n",
    "def main(is_train=True):\n",
    "    if is_train:\n",
    "        train()\n",
    "    else:\n",
    "        test()\n",
    "\n",
    "main(is_train=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
