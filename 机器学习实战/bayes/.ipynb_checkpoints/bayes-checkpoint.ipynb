{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'I', 'steak', 'how', 'help', 'mr', 'maybe', 'food', 'worthless', 'flea', 'stop', 'cute', 'ate', 'is', 'has', 'licks', 'park', 'please', 'quit', 'problems', 'not', 'buying', 'my', 'dog', 'dalmation', 'love', 'to', 'posting', 'him', 'stupid', 'take', 'garbage']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# 使用贝叶斯算法进行文本分类，对网站留言评论进行分类：侮辱类和非侮辱类\n",
    "def loadDataSet():\n",
    "    # 实验样本、文档list\n",
    "    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 样本类别，1表示是侮辱性的、0表示非侮辱性的留言\n",
    "    return postingList, classVec\n",
    "\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  # 创建空集合，set也会去除重复的元素\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) # |为按位或运算符，取两个集合的并集\n",
    "    return list(vocabSet)  # set类型是无序的，所以每次运行返回的顺序不同\n",
    "\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    # 初始化一个长度相同，全为0的list\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    # 遍历给定样本的词汇\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:  # 判断并对出现的词汇设置8为1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec\n",
    "\n",
    "post_list, classvec = loadDataSet()\n",
    "not_repeatlist = createVocabList(post_list)\n",
    "print(not_repeatlist)\n",
    "out = setOfWords2Vec(not_repeatlist, post_list[3])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练、分类\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)  # P(c)\n",
    "    # 运用拉普拉斯平滑，避免了其中一个概率值为0导致整个式子为0的情况\n",
    "    # 拉普拉斯平滑原理，分子每一项加上一个大于0的数，eg：+1，分母的所有项也+1(因为是伯努利分布，只有两项，所以分母+2)\n",
    "    p0Num = np.ones(numWords); p1Num = np.ones(numWords)      \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        \n",
    "    for i in range(numTrainDocs):\n",
    "        # 当为侮辱性留言时\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]  # 叠加词汇出现的次数\n",
    "            p1Denom += sum(trainMatrix[i])  # 总数，分母，用于求频率即概率\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 使用log避免下溢出的情况(大量很小的数相乘最终python会四舍五入输出为0)\n",
    "    # 两个条件概率\n",
    "    p1Vect = np.log(p1Num/p1Denom)         \n",
    "    p0Vect = np.log(p0Num/p0Denom)          \n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了防止分子的每一个相乘的子项不为0，则分子的条件概率变为：$$P_{\\lambda }(X^{j}=a_{jl}|Y=c_{k})=\\frac{\\sum_{N}^{i=1}I(x_{i}^{j}=a_{jl},y_{i}=c_{k})+\\lambda }{\\sum_{N}^{i=1}I(y_{i}=c_{k})+S_{j}\\lambda }$$I为指示函数，$S_{j}$代表种类的个数，具体参考《概率论与数理统计》浙大第四版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该留言类别是： 0\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    # 因为分子运用了log运算，所以sum和+是合理的\n",
    "    # 其实p1和p0不是真正的概率值，也不需要求出真实概率值，可以判断大小即可，你会发现下面两个式子中连分母(P(w))都没有用到，因为分母都一样比较分子即可\n",
    "    p1 = sum(vec2Classify * p1Vec) + np.log(pClass1)    \n",
    "    p0 = sum(vec2Classify * p0Vec) + np.log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "post_list, classvec = loadDataSet()\n",
    "not_repeatlist = createVocabList(post_list)\n",
    "traindata = []\n",
    "for postindoc in post_list:\n",
    "    traindata.append(setOfWords2Vec(not_repeatlist, postindoc))\n",
    "p1, p2, p3 = trainNB0(traindata, classvec)\n",
    "test0 = ['love', 'my', 'dalmation']\n",
    "test2num = setOfWords2Vec(not_repeatlist, test0)\n",
    "out = classifyNB(test2num, p1, p2, p3)\n",
    "print(\"该留言类别是：\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "# 没什么~就是一个测试样本的封装函数\n",
    "def testingNB():\n",
    "    listOPosts, listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(np.array(trainMat), np.array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print(testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    \n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# # 当词汇在文本中出现不止一次时，使用朴素贝叶斯词袋模型\n",
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            # 词袋模型对出现的词汇进行次数累加\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n",
    "\n",
    "# 电子邮件垃圾过滤\n",
    "###################################################\n",
    "def textParse(bigString):  # 从文本文档构建词列表\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "\n",
    "def spamTest():\n",
    "    docList = [] # 文本列表\n",
    "    classList = [] # 类别列表\n",
    "    fullText = []\n",
    "    # 加载读取data\n",
    "    for i in range(1, 26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    # 去除重复的列表\n",
    "    vocabList = createVocabList(docList)  \n",
    "    trainingSet = list(range(50))\n",
    "    testSet = []  # create test set\n",
    "    # 随机选取10个作为测试样本\n",
    "    for i in range(10):\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del trainingSet[randIndex]  # 并将测试样本从数据集中删除\n",
    "    trainMat = []\n",
    "    trainClasses = []\n",
    "    for docIndex in trainingSet:  \n",
    "        # 转换成数字向量\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    # 训练\n",
    "    p0V, p1V, pSpam = trainNB0(np.array(trainMat), np.array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:  # 测试\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        # 利用训练集求出的三个概率值测试test样本\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error\", docList[docIndex])\n",
    "    print('the error rate is: ', float(errorCount) / len(testSet))\n",
    "    # return vocabList,fullText\n",
    "\n",
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "the error rate is:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "spamTest()   # 测试样本是随机选出的，错误率不同也合理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
